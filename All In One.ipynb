{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T09:25:06.636037Z",
     "start_time": "2025-11-08T09:25:02.636966Z"
    }
   },
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: Setup, Imports, and Global Configuration\n",
    "# ==============================================================================\n",
    "\n",
    "# Step 0: Install necessary packages quietly\n",
    "!pip install pandas openpyxl xgboost tensorflow > /dev/null\n",
    "print(\"✅ Packages installed successfully.\")\n",
    "\n",
    "# Step 1: Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.feature_selection import f_classif, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    log_loss,\n",
    ")\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# TensorFlow / Keras imports\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "except ImportError:\n",
    "    print(\"TensorFlow is not installed. The MLP model will be skipped.\")\n",
    "    tf = None\n",
    "\n",
    "# Global settings for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if tf is not None:\n",
    "    tf.random.set_seed(SEED)\n",
    "    # The line below is deprecated and may cause issues in newer TF versions, but kept for legacy compatibility.\n",
    "    # For modern TF, tf.random.set_seed() is generally sufficient for determinism.\n",
    "    # tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories to store results\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"feature_selected_csvs\", exist_ok=True)\n",
    "\n",
    "print(\"✅ Libraries imported and global settings configured.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Packages installed successfully.\n",
      "✅ Libraries imported and global settings configured.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:31:13.407485Z",
     "start_time": "2025-11-08T09:31:13.401483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: Data Preprocessing Function\n",
    "# ==============================================================================\n",
    "def preprocess_dataframe(df_in):\n",
    "    \"\"\"\n",
    "    Performs essential preprocessing: one-hot encoding for categoricals\n",
    "    and median imputation for missing numerical values.\n",
    "    \"\"\"\n",
    "    print(\"--- Running initial data preprocessing ---\")\n",
    "    try:\n",
    "        # Find the target column, ignoring case and whitespace\n",
    "        target_col = next((c for c in df_in.columns if c.strip().lower() == \"target\"), None)\n",
    "        if target_col is None:\n",
    "            raise KeyError(\"Couldn't find 'Target' column for preprocessing.\")\n",
    "\n",
    "        X = df_in.drop(columns=[target_col])\n",
    "        y = df_in[target_col]\n",
    "\n",
    "        # One-hot encode categorical features (if any exist)\n",
    "        X_enc = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "        # Impute missing values with the median for all numeric columns\n",
    "        X_enc = X_enc.replace([np.inf, -np.inf], np.nan).fillna(X_enc.median(numeric_only=True))\n",
    "\n",
    "        # Recombine features and target\n",
    "        df_processed = X_enc.copy()\n",
    "        df_processed[target_col] = y.values\n",
    "        print(f\"Preprocessing complete. Shape changed from {df_in.shape} to {df_processed.shape}.\")\n",
    "        return df_processed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during preprocessing: {e}\")\n",
    "        return None"
   ],
   "id": "b15d5a32ac7af40",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:33:23.348023Z",
     "start_time": "2025-11-08T09:33:23.338915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Feature Selection Functions\n",
    "# ==============================================================================\n",
    "def run_anova_selection(df_in, eta2_thresh, omega2_thresh):\n",
    "    \"\"\"Performs ANOVA F-test feature selection and saves the result.\"\"\"\n",
    "    out_path = f\"feature_selected_csvs/FS_ANOVA_eta2_{eta2_thresh}_omega2_{omega2_thresh}.csv\"\n",
    "    print(f\"\\n--- Running ANOVA (eta2>{eta2_thresh}, omega2>{omega2_thresh}) ---\")\n",
    "    try:\n",
    "        target_col = next((c for c in df_in.columns if c.strip().lower() == \"target\"), None)\n",
    "        if target_col is None: raise KeyError(\"Couldn't find 'Target' column.\")\n",
    "        X_enc = df_in.drop(columns=[target_col]) # Data is already encoded\n",
    "        y = pd.to_numeric(df_in[target_col], errors=\"raise\").astype(int)\n",
    "        F, p = f_classif(X_enc, y)\n",
    "        k, n = y.nunique(), len(y)\n",
    "        df_between, df_within = k - 1, n - k\n",
    "        eta2 = (F * df_between) / (F * df_between + df_within)\n",
    "        omega2 = np.clip((df_between * (F - 1)) / (df_between * F + df_within + 1), 0, None)\n",
    "        metrics = pd.DataFrame({\"feature\": X_enc.columns, \"eta2\": eta2, \"omega2\": omega2})\n",
    "        selected = metrics.query(f\"eta2 > {eta2_thresh} and omega2 > {omega2_thresh}\")[\"feature\"].tolist()\n",
    "        if not selected:\n",
    "            print(f\"Result: No features selected. Skipping file generation.\")\n",
    "            return None, 0\n",
    "        df_out = X_enc[selected].copy()\n",
    "        df_out[target_col] = y.values\n",
    "        df_out.to_csv(out_path, sep=\";\", index=False)\n",
    "        print(f\"Result: Selected {len(selected)} features. Saved to {out_path}\")\n",
    "        return out_path, len(selected)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ANOVA selection: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def run_pearson_selection(df_in, target_corr_thresh):\n",
    "    \"\"\"Performs Pearson correlation-based feature selection.\"\"\"\n",
    "    out_path = f\"feature_selected_csvs/FS_Pearson_targetCorr_{target_corr_thresh}.csv\"\n",
    "    print(f\"\\n--- Running Pearson's (|r|>{target_corr_thresh}) ---\")\n",
    "    try:\n",
    "        df = df_in.copy()\n",
    "        target_col = next((c for c in df.columns if c.strip().lower() == \"target\"), None)\n",
    "        if target_col is None: raise KeyError(\"Couldn't find 'Target' column.\")\n",
    "        df[target_col] = pd.to_numeric(df[target_col], errors='coerce').fillna(-1).astype(int)\n",
    "        df = df[df[target_col].isin([0, 1])].copy()\n",
    "        y = df[target_col]\n",
    "        X_enc = df.drop(columns=[target_col]) # Data is already encoded\n",
    "        X_enc = X_enc.loc[:, X_enc.nunique() > 1]\n",
    "        corr_with_target = X_enc.corrwith(y).abs()\n",
    "        selected = corr_with_target[corr_with_target >= target_corr_thresh].index.tolist()\n",
    "        if not selected:\n",
    "            print(f\"Result: No features selected. Skipping file generation.\")\n",
    "            return None, 0\n",
    "        df_out = X_enc[selected].copy()\n",
    "        df_out[target_col] = y.values\n",
    "        df_out.to_csv(out_path, sep=\";\", index=False)\n",
    "        print(f\"Result: Selected {len(selected)} features. Saved to {out_path}\")\n",
    "        return out_path, len(selected)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Pearson selection: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def run_chi2_selection(df_in, p_value_thresh):\n",
    "    \"\"\"Performs Chi-Square feature selection.\"\"\"\n",
    "    out_path = f\"feature_selected_csvs/FS_ChiSquare_p_{p_value_thresh}.csv\"\n",
    "    print(f\"\\n--- Running Chi-Square (p<{p_value_thresh}) ---\")\n",
    "    try:\n",
    "        target_col = next((c for c in df_in.columns if c.strip().lower() == \"target\"), None)\n",
    "        if target_col is None: raise KeyError(\"Couldn't find 'Target' column.\")\n",
    "        X = df_in.drop(columns=[target_col]) # Data is already encoded/numeric\n",
    "        y = df_in[target_col]\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "        _, p_values = chi2(X_scaled, y)\n",
    "        results = pd.DataFrame({\"Feature\": X.columns, \"p-value\": p_values})\n",
    "        selected = results[results[\"p-value\"] < p_value_thresh][\"Feature\"].tolist()\n",
    "        if not selected:\n",
    "            print(f\"Result: No features selected. Skipping file generation.\")\n",
    "            return None, 0\n",
    "        df_out = X[selected].copy()\n",
    "        df_out[target_col] = y.values\n",
    "        df_out.to_csv(out_path, sep=\";\", index=False)\n",
    "        print(f\"Result: Selected {len(selected)} features. Saved to {out_path}\")\n",
    "        return out_path, len(selected)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Chi-Square selection: {e}\")\n",
    "        return None, 0"
   ],
   "id": "db49af6693418ffc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:45:40.890605Z",
     "start_time": "2025-11-08T09:45:40.879994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: Machine Learning Model Functions (Scikit-learn & XGBoost)\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_sklearn_metrics(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"A helper function for scikit-learn models.\"\"\"\n",
    "    results = {}\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    results['Train_Accuracy'] = accuracy_score(y_train, y_train_pred)\n",
    "    try:\n",
    "        y_train_proba = model.predict_proba(X_train)\n",
    "        results['Train_LogLoss'] = log_loss(y_train, y_train_proba)\n",
    "        results['Train_ROC_AUC'] = roc_auc_score(y_train, y_train_proba[:, 1])\n",
    "    except Exception:\n",
    "        results['Train_LogLoss'], results['Train_ROC_AUC'] = None, None\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    results['Test_Accuracy'] = accuracy_score(y_test, y_test_pred)\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test)\n",
    "        results['Test_LogLoss'] = log_loss(y_test, y_test_proba)\n",
    "        results['Test_ROC_AUC'] = roc_auc_score(y_test, y_test_proba[:, 1])\n",
    "    except Exception:\n",
    "        results['Test_LogLoss'], results['Test_ROC_AUC'] = None, None\n",
    "    results['Test_Classification_Report'] = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    results['Test_Confusion_Matrix'] = str(confusion_matrix(y_test, y_test_pred).tolist())\n",
    "    return results\n",
    "\n",
    "def train_evaluate_rf(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates a RandomForestClassifier with robust anti-overfitting parameters.\"\"\"\n",
    "    print(\"Training RandomForest...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=8, min_samples_leaf=5,\n",
    "        min_samples_split=10, max_features='sqrt', class_weight=\"balanced\",\n",
    "        random_state=SEED, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return calculate_sklearn_metrics(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_evaluate_dt(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates a DecisionTreeClassifier with GridSearchCV.\"\"\"\n",
    "    print(\"Training Decision Tree with GridSearchCV (this may take a while)...\")\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", DecisionTreeClassifier(random_state=SEED))])\n",
    "    param_grid = {\"clf__max_depth\": [5, 10], \"clf__min_samples_split\": [2, 10], \"clf__class_weight\": [None, \"balanced\"]}\n",
    "    grid = GridSearchCV(pipe, param_grid, scoring=\"f1_weighted\", cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return calculate_sklearn_metrics(grid.best_estimator_, X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_evaluate_lr(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates a LogisticRegression model.\"\"\"\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('model', LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=SEED))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    return calculate_sklearn_metrics(pipe, X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_evaluate_xgb(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates an XGBClassifier with robust anti-overfitting parameters.\"\"\"\n",
    "    print(\"Training XGBoost...\")\n",
    "    neg, pos = np.sum(y_train == 0), np.sum(y_train == 1)\n",
    "    scale_pos_weight = neg/pos if pos > 0 else 1\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\", use_label_encoder=False, eval_metric='logloss',\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=5, subsample=0.8,\n",
    "        colsample_bytree=0.8, gamma=0.1, reg_alpha=0.1, reg_lambda=0.1,\n",
    "        random_state=SEED, n_jobs=-1, scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return calculate_sklearn_metrics(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "def train_evaluate_svm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates an SVC model.\"\"\"\n",
    "    print(\"Training SVM...\")\n",
    "    pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('model', SVC(class_weight='balanced', probability=True, random_state=SEED))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    return calculate_sklearn_metrics(pipe, X_train, y_train, X_test, y_test)"
   ],
   "id": "87fc1140d6c690a1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:41:05.157848Z",
     "start_time": "2025-11-08T09:41:05.151489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: Machine Learning Model Function (TensorFlow/Keras)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_evaluate_mlp(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains and evaluates a Multi-Layer Perceptron with 100% reproducibility.\"\"\"\n",
    "    if tf is None: return {}\n",
    "    print(\"Training MLP (TensorFlow) with deterministic settings...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Re-set seed specifically for Keras model construction\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(128, activation='relu'), layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'), layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=100,\n",
    "              batch_size=64, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    results = {}\n",
    "    train_loss, train_acc, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    results['Train_Accuracy'], results['Train_LogLoss'], results['Train_ROC_AUC'] = train_acc, train_loss, train_auc\n",
    "    test_loss, test_acc, test_auc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    results['Test_Accuracy'], results['Test_LogLoss'], results['Test_ROC_AUC'] = test_acc, test_loss, test_auc\n",
    "    y_test_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "    y_test_pred_labels = (y_test_pred_proba > 0.5).astype(\"int32\")\n",
    "    results['Test_Classification_Report'] = classification_report(y_test, y_test_pred_labels, output_dict=True)\n",
    "    results['Test_Confusion_Matrix'] = str(confusion_matrix(y_test, y_test_pred_labels).tolist())\n",
    "    return results"
   ],
   "id": "21fb6582655f9e06",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:57:18.930555Z",
     "start_time": "2025-11-08T09:56:02.968837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: Main Execution Workflow\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the entire workflow.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"      COMPREHENSIVE FEATURE SELECTION & ML PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nSTEP 1: UPLOAD YOUR DATASET\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Please use the button below to upload your CSV file.\")\n",
    "\n",
    "    input_filename = r\"data/dropoutgraduate.csv\"\n",
    "    print(f\"\\n✅ Successfully uploaded '{input_filename}'.\")\n",
    "    try:\n",
    "        df_original = pd.read_csv(input_filename, sep=';')\n",
    "    except Exception:\n",
    "        try:\n",
    "            df_original = pd.read_csv(input_filename, sep=',')\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error: Could not read the CSV file. Details: {e}\")\n",
    "            return\n",
    "\n",
    "    # Preprocess data ONCE at the beginning\n",
    "    df_processed = preprocess_dataframe(df_original.copy())\n",
    "    if df_processed is None:\n",
    "        print(\"Halting execution due to preprocessing error.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nSTEP 2: SETTING UP DATASETS FOR TRAINING\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    generated_files = []\n",
    "    try:\n",
    "        original_data_path = \"feature_selected_csvs/FS_None_All_Features.csv\"\n",
    "        # Save the PROCESSED dataframe for the baseline run\n",
    "        df_processed.to_csv(original_data_path, sep=\";\", index=False)\n",
    "        target_col_name = next((c for c in df_processed.columns if c.strip().lower() == \"target\"), None)\n",
    "        num_processed_features = len(df_processed.columns) - 1\n",
    "        generated_files.append({\n",
    "            \"path\": original_data_path, \"method\": \"None\",\n",
    "            \"thresholds\": \"All Features\", \"num_features\": num_processed_features\n",
    "        })\n",
    "        print(\"\\n--- Baseline Added: Using all preprocessed features ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error setting up the 'no feature selection' baseline: {e}\")\n",
    "\n",
    "    anova_thresholds = [(0.01, 0.01), (0.04, 0.04), (0.06, 0.06), (0.08, 0.08), (0.14, 0.14)]\n",
    "    pearson_thresholds = [0.05, 0.08, 0.10, 0.15, 0.30]\n",
    "    chi2_thresholds = [0.1, 0.07, 0.05, 0.03, 0.01]\n",
    "\n",
    "    # Run feature selection on the preprocessed dataframe\n",
    "    for eta2, omega2 in anova_thresholds:\n",
    "        path, count = run_anova_selection(df_processed.copy(), eta2, omega2)\n",
    "        if path: generated_files.append({\"path\": path, \"method\": \"ANOVA\", \"thresholds\": f\"eta2>{eta2}, omega2>{omega2}\", \"num_features\": count})\n",
    "    for thresh in pearson_thresholds:\n",
    "        path, count = run_pearson_selection(df_processed.copy(), thresh)\n",
    "        if path: generated_files.append({\"path\": path, \"method\": \"Pearson\", \"thresholds\": f\"|r|>{thresh}\", \"num_features\": count})\n",
    "    for thresh in chi2_thresholds:\n",
    "        path, count = run_chi2_selection(df_processed.copy(), thresh)\n",
    "        if path: generated_files.append({\"path\": path, \"method\": \"Chi-Square\", \"thresholds\": f\"p<{thresh}\", \"num_features\": count})\n",
    "\n",
    "    if not generated_files:\n",
    "        print(\"\\n❌ Error: No datasets to process. Halting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n✅ Setup complete. Processing {len(generated_files)} unique datasets (including baseline).\")\n",
    "\n",
    "    print(\"\\nSTEP 3: TRAINING AND EVALUATING ML MODELS\")\n",
    "    print(\"-\" * 60)\n",
    "    all_results = []\n",
    "    models_to_run = {\n",
    "        \"RandomForest\": train_evaluate_rf, \"DecisionTree_GridSearchCV\": train_evaluate_dt,\n",
    "        \"LogisticRegression\": train_evaluate_lr, \"XGBoost\": train_evaluate_xgb,\n",
    "        \"SVM\": train_evaluate_svm, \"MLP_TensorFlow\": train_evaluate_mlp,\n",
    "    }\n",
    "    total_tasks = len(generated_files) * len(models_to_run)\n",
    "    tasks_completed = 0\n",
    "    overall_start_time = time.time()\n",
    "    for i, file_info in enumerate(generated_files):\n",
    "        print(f\"\\nProcessing Dataset {i+1}/{len(generated_files)}: [Method: {file_info['method']}, Thresholds: {file_info['thresholds']}]\")\n",
    "        df_fs = pd.read_csv(file_info['path'], sep=\";\")\n",
    "        target_col = next((c for c in df_fs.columns if c.strip().lower() == \"target\"), None)\n",
    "        X = df_fs.drop(columns=[target_col])\n",
    "        y = pd.to_numeric(df_fs[target_col], errors='coerce').fillna(-1).astype(int)\n",
    "        valid_indices = y.isin([0, 1])\n",
    "        X, y = X[valid_indices], y[valid_indices]\n",
    "        if len(y.unique()) < 2:\n",
    "            print(f\"  Skipping this dataset as it contains only one class after filtering.\")\n",
    "            tasks_completed += len(models_to_run)\n",
    "            continue\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "        for model_name, train_func in models_to_run.items():\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                metrics = train_func(X_train.copy(), y_train.copy(), X_test.copy(), y_test.copy())\n",
    "                if not metrics:\n",
    "                    tasks_completed += 1\n",
    "                    continue\n",
    "                report = metrics.pop('Test_Classification_Report', {})\n",
    "                for class_label, scores in report.items():\n",
    "                    if isinstance(scores, dict):\n",
    "                        for metric, value in scores.items():\n",
    "                            metrics[f'Test_Report_{class_label}_{metric}'] = value\n",
    "                    else:\n",
    "                        metrics[f'Test_Report_{class_label}'] = scores\n",
    "                result_row = {\"FeatureSelectionMethod\": file_info[\"method\"], \"Thresholds\": file_info[\"thresholds\"],\n",
    "                              \"NumSelectedFeatures\": file_info[\"num_features\"], \"ML_Algorithm\": model_name, **metrics}\n",
    "                all_results.append(result_row)\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ ERROR running {model_name} on {file_info['path']}: {e}\")\n",
    "            finally:\n",
    "                tasks_completed += 1\n",
    "                elapsed_time = time.time() - overall_start_time\n",
    "                avg_time_per_task = elapsed_time / tasks_completed if tasks_completed > 0 else 0\n",
    "                remaining_tasks = total_tasks - tasks_completed\n",
    "                eta_seconds = remaining_tasks * avg_time_per_task\n",
    "                eta_minutes, eta_sec_rem = divmod(eta_seconds, 60)\n",
    "                eta_str = f\"{int(eta_minutes)}m {int(eta_sec_rem)}s\"\n",
    "                model_time = time.time() - start_time\n",
    "                print(f\"  [{tasks_completed}/{total_tasks}] Finished {model_name} in {model_time:.2f}s. | ETA: {eta_str}\")\n",
    "\n",
    "    print(\"\\nSTEP 4: SAVING FINAL RESULTS\")\n",
    "    print(\"-\" * 60)\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        cols_order = [\"FeatureSelectionMethod\", \"Thresholds\", \"NumSelectedFeatures\", \"ML_Algorithm\",\n",
    "                      \"Train_Accuracy\", \"Test_Accuracy\", \"Train_LogLoss\", \"Test_LogLoss\",\n",
    "                      \"Train_ROC_AUC\", \"Test_ROC_AUC\", \"Test_Confusion_Matrix\"]\n",
    "        report_cols = sorted([c for c in results_df.columns if c.startswith('Test_Report_')])\n",
    "        final_cols = cols_order + report_cols\n",
    "        results_df = results_df.reindex(columns=final_cols).fillna('')\n",
    "        output_excel_path = \"data/ML_Evaluation_Results.xlsx\"\n",
    "        results_df.to_excel(output_excel_path, index=False)\n",
    "        print(f\"\\n✅ All processing complete! Results saved to '{output_excel_path}'.\")\n",
    "    else:\n",
    "        print(\"\\nNo models were successfully trained. No results file generated.\")\n",
    "\n",
    "# This line calls the main function to start the script.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "f37c6f00fc8603c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "      COMPREHENSIVE FEATURE SELECTION & ML PIPELINE\n",
      "============================================================\n",
      "\n",
      "STEP 1: UPLOAD YOUR DATASET\n",
      "------------------------------------------------------------\n",
      "Please use the button below to upload your CSV file.\n",
      "\n",
      "✅ Successfully uploaded 'data/dropoutgraduate.csv'.\n",
      "--- Running initial data preprocessing ---\n",
      "Preprocessing complete. Shape changed from (3630, 37) to (3630, 37).\n",
      "\n",
      "STEP 2: SETTING UP DATASETS FOR TRAINING\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Baseline Added: Using all preprocessed features ---\n",
      "\n",
      "--- Running ANOVA (eta2>0.01, omega2>0.01) ---\n",
      "Result: Selected 17 features. Saved to feature_selected_csvs/FS_ANOVA_eta2_0.01_omega2_0.01.csv\n",
      "\n",
      "--- Running ANOVA (eta2>0.04, omega2>0.04) ---\n",
      "Result: Selected 10 features. Saved to feature_selected_csvs/FS_ANOVA_eta2_0.04_omega2_0.04.csv\n",
      "\n",
      "--- Running ANOVA (eta2>0.06, omega2>0.06) ---\n",
      "Result: Selected 9 features. Saved to feature_selected_csvs/FS_ANOVA_eta2_0.06_omega2_0.06.csv\n",
      "\n",
      "--- Running ANOVA (eta2>0.08, omega2>0.08) ---\n",
      "Result: Selected 6 features. Saved to feature_selected_csvs/FS_ANOVA_eta2_0.08_omega2_0.08.csv\n",
      "\n",
      "--- Running ANOVA (eta2>0.14, omega2>0.14) ---\n",
      "Result: Selected 5 features. Saved to feature_selected_csvs/FS_ANOVA_eta2_0.14_omega2_0.14.csv\n",
      "\n",
      "--- Running Pearson's (|r|>0.05) ---\n",
      "Result: Selected 26 features. Saved to feature_selected_csvs/FS_Pearson_targetCorr_0.05.csv\n",
      "\n",
      "--- Running Pearson's (|r|>0.08) ---\n",
      "Result: Selected 20 features. Saved to feature_selected_csvs/FS_Pearson_targetCorr_0.08.csv\n",
      "\n",
      "--- Running Pearson's (|r|>0.1) ---\n",
      "Result: Selected 18 features. Saved to feature_selected_csvs/FS_Pearson_targetCorr_0.1.csv\n",
      "\n",
      "--- Running Pearson's (|r|>0.15) ---\n",
      "Result: Selected 12 features. Saved to feature_selected_csvs/FS_Pearson_targetCorr_0.15.csv\n",
      "\n",
      "--- Running Pearson's (|r|>0.3) ---\n",
      "Result: Selected 6 features. Saved to feature_selected_csvs/FS_Pearson_targetCorr_0.3.csv\n",
      "\n",
      "--- Running Chi-Square (p<0.1) ---\n",
      "Result: Selected 24 features. Saved to feature_selected_csvs/FS_ChiSquare_p_0.1.csv\n",
      "\n",
      "--- Running Chi-Square (p<0.07) ---\n",
      "Result: Selected 20 features. Saved to feature_selected_csvs/FS_ChiSquare_p_0.07.csv\n",
      "\n",
      "--- Running Chi-Square (p<0.05) ---\n",
      "Result: Selected 18 features. Saved to feature_selected_csvs/FS_ChiSquare_p_0.05.csv\n",
      "\n",
      "--- Running Chi-Square (p<0.03) ---\n",
      "Result: Selected 16 features. Saved to feature_selected_csvs/FS_ChiSquare_p_0.03.csv\n",
      "\n",
      "--- Running Chi-Square (p<0.01) ---\n",
      "Result: Selected 14 features. Saved to feature_selected_csvs/FS_ChiSquare_p_0.01.csv\n",
      "\n",
      "✅ Setup complete. Processing 16 unique datasets (including baseline).\n",
      "\n",
      "STEP 3: TRAINING AND EVALUATING ML MODELS\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing Dataset 1/16: [Method: None, Thresholds: All Features]\n",
      "Training RandomForest...\n",
      "  [1/96] Finished RandomForest in 0.33s. | ETA: 0m 32s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [2/96] Finished DecisionTree_GridSearchCV in 0.10s. | ETA: 0m 20s\n",
      "Training Logistic Regression...\n",
      "  [3/96] Finished LogisticRegression in 0.03s. | ETA: 0m 14s\n",
      "Training XGBoost...\n",
      "  [4/96] Finished XGBoost in 0.19s. | ETA: 0m 15s\n",
      "Training SVM...\n",
      "  [5/96] Finished SVM in 1.17s. | ETA: 0m 33s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [6/96] Finished MLP_TensorFlow in 2.98s. | ETA: 1m 12s\n",
      "\n",
      "Processing Dataset 2/16: [Method: ANOVA, Thresholds: eta2>0.01, omega2>0.01]\n",
      "Training RandomForest...\n",
      "  [7/96] Finished RandomForest in 0.33s. | ETA: 1m 5s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [8/96] Finished DecisionTree_GridSearchCV in 0.08s. | ETA: 0m 57s\n",
      "Training Logistic Regression...\n",
      "  [9/96] Finished LogisticRegression in 0.02s. | ETA: 0m 50s\n",
      "Training XGBoost...\n",
      "  [10/96] Finished XGBoost in 0.15s. | ETA: 0m 46s\n",
      "Training SVM...\n",
      "  [11/96] Finished SVM in 0.87s. | ETA: 0m 48s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [12/96] Finished MLP_TensorFlow in 2.83s. | ETA: 1m 3s\n",
      "\n",
      "Processing Dataset 3/16: [Method: ANOVA, Thresholds: eta2>0.04, omega2>0.04]\n",
      "Training RandomForest...\n",
      "  [13/96] Finished RandomForest in 0.33s. | ETA: 1m 0s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [14/96] Finished DecisionTree_GridSearchCV in 0.05s. | ETA: 0m 55s\n",
      "Training Logistic Regression...\n",
      "  [15/96] Finished LogisticRegression in 0.02s. | ETA: 0m 51s\n",
      "Training XGBoost...\n",
      "  [16/96] Finished XGBoost in 0.12s. | ETA: 0m 48s\n",
      "Training SVM...\n",
      "  [17/96] Finished SVM in 0.84s. | ETA: 0m 48s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [18/96] Finished MLP_TensorFlow in 3.89s. | ETA: 1m 2s\n",
      "\n",
      "Processing Dataset 4/16: [Method: ANOVA, Thresholds: eta2>0.06, omega2>0.06]\n",
      "Training RandomForest...\n",
      "  [19/96] Finished RandomForest in 0.34s. | ETA: 0m 59s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [20/96] Finished DecisionTree_GridSearchCV in 0.06s. | ETA: 0m 56s\n",
      "Training Logistic Regression...\n",
      "  [21/96] Finished LogisticRegression in 0.02s. | ETA: 0m 52s\n",
      "Training XGBoost...\n",
      "  [22/96] Finished XGBoost in 0.13s. | ETA: 0m 50s\n",
      "Training SVM...\n",
      "  [23/96] Finished SVM in 0.83s. | ETA: 0m 50s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [24/96] Finished MLP_TensorFlow in 3.47s. | ETA: 0m 57s\n",
      "\n",
      "Processing Dataset 5/16: [Method: ANOVA, Thresholds: eta2>0.08, omega2>0.08]\n",
      "Training RandomForest...\n",
      "  [25/96] Finished RandomForest in 0.33s. | ETA: 0m 55s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [26/96] Finished DecisionTree_GridSearchCV in 0.05s. | ETA: 0m 52s\n",
      "Training Logistic Regression...\n",
      "  [27/96] Finished LogisticRegression in 0.02s. | ETA: 0m 50s\n",
      "Training XGBoost...\n",
      "  [28/96] Finished XGBoost in 0.11s. | ETA: 0m 47s\n",
      "Training SVM...\n",
      "  [29/96] Finished SVM in 0.75s. | ETA: 0m 47s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [30/96] Finished MLP_TensorFlow in 2.72s. | ETA: 0m 51s\n",
      "\n",
      "Processing Dataset 6/16: [Method: ANOVA, Thresholds: eta2>0.14, omega2>0.14]\n",
      "Training RandomForest...\n",
      "  [31/96] Finished RandomForest in 0.33s. | ETA: 0m 49s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [32/96] Finished DecisionTree_GridSearchCV in 0.05s. | ETA: 0m 47s\n",
      "Training Logistic Regression...\n",
      "  [33/96] Finished LogisticRegression in 0.02s. | ETA: 0m 45s\n",
      "Training XGBoost...\n",
      "  [34/96] Finished XGBoost in 0.11s. | ETA: 0m 43s\n",
      "Training SVM...\n",
      "  [35/96] Finished SVM in 0.74s. | ETA: 0m 42s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [36/96] Finished MLP_TensorFlow in 2.84s. | ETA: 0m 45s\n",
      "\n",
      "Processing Dataset 7/16: [Method: Pearson, Thresholds: |r|>0.05]\n",
      "Training RandomForest...\n",
      "  [37/96] Finished RandomForest in 0.34s. | ETA: 0m 44s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [38/96] Finished DecisionTree_GridSearchCV in 0.08s. | ETA: 0m 42s\n",
      "Training Logistic Regression...\n",
      "  [39/96] Finished LogisticRegression in 0.02s. | ETA: 0m 40s\n",
      "Training XGBoost...\n",
      "  [40/96] Finished XGBoost in 0.17s. | ETA: 0m 39s\n",
      "Training SVM...\n",
      "  [41/96] Finished SVM in 1.07s. | ETA: 0m 38s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [42/96] Finished MLP_TensorFlow in 2.95s. | ETA: 0m 41s\n",
      "\n",
      "Processing Dataset 8/16: [Method: Pearson, Thresholds: |r|>0.08]\n",
      "Training RandomForest...\n",
      "  [43/96] Finished RandomForest in 0.33s. | ETA: 0m 39s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [44/96] Finished DecisionTree_GridSearchCV in 0.07s. | ETA: 0m 38s\n",
      "Training Logistic Regression...\n",
      "  [45/96] Finished LogisticRegression in 0.02s. | ETA: 0m 36s\n",
      "Training XGBoost...\n",
      "  [46/96] Finished XGBoost in 0.15s. | ETA: 0m 35s\n",
      "Training SVM...\n",
      "  [47/96] Finished SVM in 0.97s. | ETA: 0m 34s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [48/96] Finished MLP_TensorFlow in 3.75s. | ETA: 0m 37s\n",
      "\n",
      "Processing Dataset 9/16: [Method: Pearson, Thresholds: |r|>0.1]\n",
      "Training RandomForest...\n",
      "  [49/96] Finished RandomForest in 0.35s. | ETA: 0m 36s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [50/96] Finished DecisionTree_GridSearchCV in 0.08s. | ETA: 0m 34s\n",
      "Training Logistic Regression...\n",
      "  [51/96] Finished LogisticRegression in 0.02s. | ETA: 0m 33s\n",
      "Training XGBoost...\n",
      "  [52/96] Finished XGBoost in 0.18s. | ETA: 0m 32s\n",
      "Training SVM...\n",
      "  [53/96] Finished SVM in 0.92s. | ETA: 0m 31s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [54/96] Finished MLP_TensorFlow in 3.21s. | ETA: 0m 32s\n",
      "\n",
      "Processing Dataset 10/16: [Method: Pearson, Thresholds: |r|>0.15]\n",
      "Training RandomForest...\n",
      "  [55/96] Finished RandomForest in 0.34s. | ETA: 0m 31s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [56/96] Finished DecisionTree_GridSearchCV in 0.06s. | ETA: 0m 30s\n",
      "Training Logistic Regression...\n",
      "  [57/96] Finished LogisticRegression in 0.02s. | ETA: 0m 29s\n",
      "Training XGBoost...\n",
      "  [58/96] Finished XGBoost in 0.13s. | ETA: 0m 27s\n",
      "Training SVM...\n",
      "  [59/96] Finished SVM in 0.81s. | ETA: 0m 27s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [60/96] Finished MLP_TensorFlow in 2.72s. | ETA: 0m 27s\n",
      "\n",
      "Processing Dataset 11/16: [Method: Pearson, Thresholds: |r|>0.3]\n",
      "Training RandomForest...\n",
      "  [61/96] Finished RandomForest in 0.34s. | ETA: 0m 26s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [62/96] Finished DecisionTree_GridSearchCV in 0.05s. | ETA: 0m 25s\n",
      "Training Logistic Regression...\n",
      "  [63/96] Finished LogisticRegression in 0.02s. | ETA: 0m 24s\n",
      "Training XGBoost...\n",
      "  [64/96] Finished XGBoost in 0.11s. | ETA: 0m 23s\n",
      "Training SVM...\n",
      "  [65/96] Finished SVM in 0.76s. | ETA: 0m 22s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [66/96] Finished MLP_TensorFlow in 2.95s. | ETA: 0m 22s\n",
      "\n",
      "Processing Dataset 12/16: [Method: Chi-Square, Thresholds: p<0.1]\n",
      "Training RandomForest...\n",
      "  [67/96] Finished RandomForest in 0.33s. | ETA: 0m 21s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [68/96] Finished DecisionTree_GridSearchCV in 0.07s. | ETA: 0m 20s\n",
      "Training Logistic Regression...\n",
      "  [69/96] Finished LogisticRegression in 0.02s. | ETA: 0m 19s\n",
      "Training XGBoost...\n",
      "  [70/96] Finished XGBoost in 0.16s. | ETA: 0m 18s\n",
      "Training SVM...\n",
      "  [71/96] Finished SVM in 1.03s. | ETA: 0m 18s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [72/96] Finished MLP_TensorFlow in 3.91s. | ETA: 0m 18s\n",
      "\n",
      "Processing Dataset 13/16: [Method: Chi-Square, Thresholds: p<0.07]\n",
      "Training RandomForest...\n",
      "  [73/96] Finished RandomForest in 0.32s. | ETA: 0m 17s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [74/96] Finished DecisionTree_GridSearchCV in 0.08s. | ETA: 0m 16s\n",
      "Training Logistic Regression...\n",
      "  [75/96] Finished LogisticRegression in 0.02s. | ETA: 0m 15s\n",
      "Training XGBoost...\n",
      "  [76/96] Finished XGBoost in 0.15s. | ETA: 0m 14s\n",
      "Training SVM...\n",
      "  [77/96] Finished SVM in 1.01s. | ETA: 0m 14s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [78/96] Finished MLP_TensorFlow in 2.83s. | ETA: 0m 13s\n",
      "\n",
      "Processing Dataset 14/16: [Method: Chi-Square, Thresholds: p<0.05]\n",
      "Training RandomForest...\n",
      "  [79/96] Finished RandomForest in 0.33s. | ETA: 0m 13s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [80/96] Finished DecisionTree_GridSearchCV in 0.07s. | ETA: 0m 12s\n",
      "Training Logistic Regression...\n",
      "  [81/96] Finished LogisticRegression in 0.02s. | ETA: 0m 11s\n",
      "Training XGBoost...\n",
      "  [82/96] Finished XGBoost in 0.15s. | ETA: 0m 10s\n",
      "Training SVM...\n",
      "  [83/96] Finished SVM in 0.92s. | ETA: 0m 9s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [84/96] Finished MLP_TensorFlow in 2.94s. | ETA: 0m 9s\n",
      "\n",
      "Processing Dataset 15/16: [Method: Chi-Square, Thresholds: p<0.03]\n",
      "Training RandomForest...\n",
      "  [85/96] Finished RandomForest in 0.33s. | ETA: 0m 8s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [86/96] Finished DecisionTree_GridSearchCV in 0.07s. | ETA: 0m 7s\n",
      "Training Logistic Regression...\n",
      "  [87/96] Finished LogisticRegression in 0.02s. | ETA: 0m 6s\n",
      "Training XGBoost...\n",
      "  [88/96] Finished XGBoost in 0.14s. | ETA: 0m 5s\n",
      "Training SVM...\n",
      "  [89/96] Finished SVM in 0.96s. | ETA: 0m 5s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [90/96] Finished MLP_TensorFlow in 4.30s. | ETA: 0m 4s\n",
      "\n",
      "Processing Dataset 16/16: [Method: Chi-Square, Thresholds: p<0.01]\n",
      "Training RandomForest...\n",
      "  [91/96] Finished RandomForest in 0.34s. | ETA: 0m 3s\n",
      "Training Decision Tree with GridSearchCV (this may take a while)...\n",
      "  [92/96] Finished DecisionTree_GridSearchCV in 0.07s. | ETA: 0m 3s\n",
      "Training Logistic Regression...\n",
      "  [93/96] Finished LogisticRegression in 0.02s. | ETA: 0m 2s\n",
      "Training XGBoost...\n",
      "  [94/96] Finished XGBoost in 0.13s. | ETA: 0m 1s\n",
      "Training SVM...\n",
      "  [95/96] Finished SVM in 1.01s. | ETA: 0m 0s\n",
      "Training MLP (TensorFlow) with deterministic settings...\n",
      "  [96/96] Finished MLP_TensorFlow in 3.28s. | ETA: 0m 0s\n",
      "\n",
      "STEP 4: SAVING FINAL RESULTS\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ All processing complete! Results saved to 'data/ML_Evaluation_Results.xlsx'.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
